import pyaudio
import wave
import openai
import pyautogui
import webbrowser
import time
import win32com.client


shell = win32com.client.Dispatch("WScript.Shell")


# Define function to get the current screenshot
def get_screenshot():
    # Take a screenshot of the current screen
    screenshot = pyautogui.screenshot()
    # Convert the screenshot to bytes
    screenshot_bytes = screenshot.tobytes()
    return screenshot_bytes

# Define function to transcribe audio to text using Whisper API
def transcribe_audio():
    # Set up PyAudio constants
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 44100
    CHUNK = 1024
    RECORD_SECONDS = 7

    # Create a PyAudio object
    p = pyaudio.PyAudio()

    # Open the microphone stream
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)

    print("Recording...")

    # Record audio data from the microphone
    frames = []
    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)

    print("Finished recording.")

    # Stop the microphone stream
    stream.stop_stream()
    stream.close()

    # Terminate the PyAudio object
    p.terminate()

    # Save the recorded audio to a WAV file
    wf = wave.open("output.wav", "wb")
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b"".join(frames))
    wf.close()

    # Initialize OpenAI API
    openai.api_key = "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"
    model_id = "whisper-1"

    # Transcribe the audio to text using Whisper API
    media_file_path = "output.wav"
    media_file = open(media_file_path, "rb")

    response = openai.Audio.transcribe(
        api_key=openai.api_key, model=model_id, file=media_file, response_format="text"
    )

    # Close the file stream
    media_file.close()

    return response.strip()

# Define function to get the text generated by Visual ChatGPT
def generate_text():
    # Get the current screenshot
    screenshot = get_screenshot()
    # Initialize OpenAI API
    openai.api_key = "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"
    model_id = "visual-chatbot-9"
    # Generate text from the screenshot using Visual ChatGPT
    response = openai.Completion.create(
        engine=model_id, prompt=screenshot, max_tokens=1024, n=1,stop=None,temperature=0.5
    )
    return response.choices[0].text.strip()


def execute_command(command):
    if 'hello' in command:
        return "Hi, how can I help you today?"
    elif 'open browser and tweet' in command:
        words = command.split()[4:]
        subject = " ".join(words)
        open_browser()
        tweet(subject)
        return f"Tweeted about {subject}!"
    else:
        return "I'm sorry, I don't understand the command."

def open_browser():
    url = 'https://twitter.com/home'
    webbrowser.open(url)
    time.sleep(5)

def tweet(subject):
    shell.SendKeys("n", 0)
    time.sleep(1)
    shell.SendKeys(subject, 0)
    time.sleep(1)
    shell.SendKeys("^{ENTER}", 0)
    time.sleep(5)


# Main loop
while True:
    # Get the user's command
    command = transcribe_audio()
    # Strip any whitespace from the command
    command = command.strip()
    # If the user didn't enter a command, skip this iteration of the loop
    if not command:
        continue
    # If the user's command contains the word "stop", break out of the loop
    if "stop" in command.lower():
        break
    # Parse the user's command and execute the appropriate action
    result = execute_command(command)
    # If the action returned a message, print it to the console
    if result:
        print(result)

# Print a message indicating that the program has stopped
print("Program stopped.")

